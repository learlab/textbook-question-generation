{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "386ad11e-a333-4d75-b31e-e5ba0445290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e1e70a8e-dcbd-4063-80a6-029f18e21e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'text', 'labels'],\n",
       "    num_rows: 3962\n",
       "})"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = '../../bin/multirc_dataset.hf'\n",
    "ds = datasets.DatasetDict.load_from_disk(dataset_path)[\"test\"]\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5ded9725-2452-4b75-b375-adbc0658765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_samples(example):\n",
    "    \"\"\"Split strings into reference/answer components,\n",
    "    so models can join them together differently.\"\"\"\n",
    "    candidate, reference = example[\"text\"].split(\"</s>\")\n",
    "    reference = candidate.strip().removeprefix(\"Answer:\").strip()\n",
    "    example[\"answer\"] = reference\n",
    "    example[\"response\"] = candidate\n",
    "    return example\n",
    "\n",
    "ds = ds.map(split_samples, remove_columns=[\"index\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0c5ad0e8-6682-408c-b928-cd7c6ae1f00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bleurt():\n",
    "    model_name = \"vaiibhavgupta/finetuned-bleurt-large\"\n",
    "    threshold = 0.7\n",
    "\n",
    "    def __init__(self):\n",
    "        self.classifier = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=self.model_name,\n",
    "            device=\"cuda\",\n",
    "        )\n",
    "\n",
    "    def __call__(self, input_dict) -> int:\n",
    "        reference = input_dict.get(\"answer\", \"\")\n",
    "        candidate = input_dict.get(\"response\", \"\")\n",
    "        \n",
    "        sequence = f\"{candidate}[SEP]{reference}\"\n",
    "\n",
    "        result = self.classifier(sequence)\n",
    "        score = result[0][\"score\"]\n",
    "\n",
    "        return 1 if score > self.threshold else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "22763d7d-5d2c-4f73-b3c4-dbac5fb2314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mpnet():\n",
    "    model_name = \"tiedaar/short-answer-classification\"\n",
    "    revision = \"77b846ec4606bfcfdf913888d7f0ab51f977a579\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.classifier = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=self.model_name,\n",
    "            revision=self.revision,\n",
    "            device=\"cuda\",\n",
    "            )\n",
    "\n",
    "    def __call__(self, input_dict) -> int:\n",
    "        reference = input_dict.get(\"answer\", \"\")\n",
    "        candidate = input_dict.get(\"response\", \"\")\n",
    "        \n",
    "        sequence = f\"{candidate}</s>{reference}\"\n",
    "\n",
    "        result = self.classifier(sequence)\n",
    "        label = result[0][\"label\"]\n",
    "\n",
    "        return 1 if label == \"correct_answer\" else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "08e0897d-3a63-4592-803c-bbf8e314b669",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModernBERT():\n",
    "    model_name = \"../../results/modernbert_multirc/\"\n",
    "    tokenizer_name = \"answerdotai/ModernBERT-base\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.classifier = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=self.model_name,\n",
    "            tokenizer=AutoTokenizer.from_pretrained(self.tokenizer_name),\n",
    "            device=\"cuda\",\n",
    "            )\n",
    "\n",
    "    def __call__(self, input_dict) -> int:\n",
    "        reference = input_dict.get(\"answer\", \"\")\n",
    "        candidate = input_dict.get(\"response\", \"\")\n",
    "\n",
    "        sequence = f\"{candidate}</s>{reference}\"\n",
    "\n",
    "        result = self.classifier(sequence)\n",
    "        label = result[0][\"label\"]\n",
    "\n",
    "        return 1 if label == \"correct\" else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5457e844-0b15-41df-b0ae-0474ca205a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "Device set to use cuda\n",
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "# NOTE: these classes are not designed to take advantage of Pipeline's batching optimizations.\n",
    "\n",
    "pipe_dict = {\n",
    "    \"Mpnet\": Mpnet(),\n",
    "    \"Bleurt\": Bleurt(),\n",
    "    \"ModernBERT\": ModernBERT(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4446c393-99c3-4a5b-b8e0-2f89018e4ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c1c972e2b648a38747f4de073c9936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9668545418484c42b409f7c0b0527cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3962 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab23a086106b472797b8bbfba5b2cbd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3962 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (209 > 128). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb8c11394374135959f693af0f85c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3962 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate_all_models(dataset, pipe_dict):\n",
    "    pred_dict = {name: [] for name in pipe_dict.keys()}\n",
    "\n",
    "    for name, pipe in tqdm(pipe_dict.items(), total=len(pipe_dict)):\n",
    "        for example in tqdm(dataset, total=len(dataset)):\n",
    "            pred_dict[name].append(pipe(example))\n",
    "\n",
    "    return pd.DataFrame(pred_dict)\n",
    "\n",
    "df = evaluate_all_models(ds, pipe_dict)\n",
    "df[\"labels\"] = ds[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "59f1808c-7362-418b-b3e0-8c78250f96c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../../data/multirc-dataset-preds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ad524815-2105-4239-8119-5d417a4acba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>ensemble</th>\n",
       "      <th>Mpnet</th>\n",
       "      <th>Bleurt</th>\n",
       "      <th>ModernBERT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.584287</td>\n",
       "      <td>0.612339</td>\n",
       "      <td>0.569871</td>\n",
       "      <td>0.572411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensemble</th>\n",
       "      <td>0.584287</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.836725</td>\n",
       "      <td>0.921361</td>\n",
       "      <td>0.723595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mpnet</th>\n",
       "      <td>0.612339</td>\n",
       "      <td>0.836725</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.741820</td>\n",
       "      <td>0.724845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bleurt</th>\n",
       "      <td>0.569871</td>\n",
       "      <td>0.921361</td>\n",
       "      <td>0.741820</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.710561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ModernBERT</th>\n",
       "      <td>0.572411</td>\n",
       "      <td>0.723595</td>\n",
       "      <td>0.724845</td>\n",
       "      <td>0.710561</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              labels  ensemble     Mpnet    Bleurt  ModernBERT\n",
       "labels      1.000000  0.584287  0.612339  0.569871    0.572411\n",
       "ensemble    0.584287  1.000000  0.836725  0.921361    0.723595\n",
       "Mpnet       0.612339  0.836725  1.000000  0.741820    0.724845\n",
       "Bleurt      0.569871  0.921361  0.741820  1.000000    0.710561\n",
       "ModernBERT  0.572411  0.723595  0.724845  0.710561    1.000000"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"ensemble\"] = (\n",
    "    (df[\"Mpnet\"] == 1)\n",
    "    | (df[\"Bleurt\"] == 1)\n",
    ").astype(int)\n",
    "\n",
    "df[[\n",
    "    \"labels\",\n",
    "    \"ensemble\",\n",
    "    \"Mpnet\",\n",
    "    \"Bleurt\",\n",
    "    \"ModernBERT\",\n",
    "]].corr(method=\"spearman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "48ae7474-8729-45b0-968a-c19b8a31e15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ensemble</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mpnet</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bleurt</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ModernBERT</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model  Accuracy  Precision  Recall  F1-Score\n",
       "0    ensemble     0.789      0.722   0.838     0.775\n",
       "1       Mpnet     0.810      0.794   0.761     0.777\n",
       "2      Bleurt     0.786      0.738   0.788     0.762\n",
       "3  ModernBERT     0.790      0.754   0.765     0.760"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "def get_metrics(df, model_names):\n",
    "    metrics = []\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        preds = df[model_name]\n",
    "\n",
    "        acc = accuracy_score(df[\"labels\"], preds)\n",
    "        p, r, f1, _ = precision_recall_fscore_support(\n",
    "            df['labels'],\n",
    "            preds,\n",
    "            average=\"binary\",\n",
    "            pos_label=1\n",
    "        )\n",
    "        \n",
    "        metrics.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': acc,\n",
    "            'Precision': p,\n",
    "            'Recall': r,\n",
    "            'F1-Score': f1\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "model_names = [\"ensemble\", \"Mpnet\", \"Bleurt\", \"ModernBERT\"]\n",
    "get_metrics(df, model_names).round(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hf]",
   "language": "python",
   "name": "conda-env-hf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
