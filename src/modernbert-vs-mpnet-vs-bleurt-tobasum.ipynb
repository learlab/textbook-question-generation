{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e4ee57-b0be-409a-909b-9a6c78244cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pyarrow\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from transformers import pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdbac7c0-5e74-4be3-8e22-9fe392f8c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac17c1c-60bd-415c-8580-af25e619fc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from compare_models_tobasum import Bleurt, Mpnet, ModernBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "689eacd4-fe4b-4b4a-a396-0e9ac7478b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "Device set to use cuda\n",
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "bleurt = Bleurt()\n",
    "mpnet = Mpnet()\n",
    "modernbert = ModernBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1404852c-1e24-484b-b449-63b28a9753e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../bin/multirc_dataset.hf\"\n",
    "\n",
    "ds = datasets.DatasetDict.load_from_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c60ed40a-968b-49f5-978c-5920b2f94835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reference answer normalization\n",
    "for sample in ds[\"test\"]:\n",
    "    try:\n",
    "        response, reference = sample[\"text\"].split(\"</s>\")\n",
    "        reference = reference.strip().removeprefix(\"Answer:\").strip()\n",
    "    except:\n",
    "        print(sample[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9cbc6de-2506-4547-b4b2-c0829edb85f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_defined_models():\n",
    "    \n",
    "    return {\n",
    "        \"ModernBERT\": modernbert,\n",
    "        \"MPNet\": mpnet,\n",
    "        \"BLEURT\": bleurt\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a455fe2-28d3-476f-ac03-007e15905754",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = get_defined_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2483283-8613-4c4a-9847-503cca8a0eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "Device set to use cuda\n",
      "Device set to use cuda\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (214 > 128). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          input_text  true_label  \\\n",
      "0  The Women's Haven of Tarrant County</s>\\nAnswe...           1   \n",
      "1  Tarrant county shelters</s>\\nAnswer: The Women...           0   \n",
      "2  Female's Safe House of Haven County</s>\\nAnswe...           0   \n",
      "3                      California</s>\\nAnswer: Texas           0   \n",
      "4                           Texas</s>\\nAnswer: Texas           1   \n",
      "\n",
      "   Mpnet_pipe_pred  Bleurt_pipe_pred  ModernBERT_pipe_pred  \n",
      "0                1                 1                     0  \n",
      "1                0                 0                     0  \n",
      "2                0                 1                     0  \n",
      "3                0                 0                     0  \n",
      "4                1                 1                     0   \n",
      "\n",
      "Mpnet_pipe:  acc=0.8107, f1_macro=0.8050\n",
      "Bleurt_pipe:  acc=0.6287, f1_macro=0.5360\n",
      "ModernBERT_pipe:  acc=0.5654, f1_macro=0.3612\n"
     ]
    }
   ],
   "source": [
    "pipes = {\n",
    "    \"Mpnet_pipe\":    Mpnet(),\n",
    "    \"Bleurt_pipe\":   Bleurt(),\n",
    "    \"ModernBERT_pipe\": ModernBERT()\n",
    "}\n",
    "\n",
    "def evaluate_and_score(dataset, pipes, label_key=\"labels\"):\n",
    "  \n",
    "    texts, true_labels = [], []\n",
    "    all_preds = { name: [] for name in pipes }\n",
    "\n",
    "    for ex in dataset[\"test\"]:\n",
    "        text  = ex[\"text\"]\n",
    "        label = ex[label_key]\n",
    "\n",
    "        texts.append(text)\n",
    "        true_labels.append(label)\n",
    "\n",
    "        for name, pipe in pipes.items():\n",
    "            \n",
    "            pred = pipe(text, ex.get(\"reference\", None))  \n",
    "            all_preds[name].append(pred)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"input_text\":  texts,\n",
    "        \"true_label\":  true_labels,\n",
    "        **{ f\"{n}_pred\": preds for n, preds in all_preds.items() }\n",
    "    })\n",
    "\n",
    "    def score(col):\n",
    "        return {\n",
    "            \"accuracy\": accuracy_score(df[\"true_label\"], df[col]),\n",
    "            \"f1_macro\":  f1_score(df[\"true_label\"], df[col], average=\"macro\")\n",
    "        }\n",
    "\n",
    "    metrics = {\n",
    "        name: score(f\"{name}_pred\")\n",
    "        for name in pipes\n",
    "    }\n",
    "\n",
    "    return df, metrics\n",
    "\n",
    "df_predictions_multirc, results = evaluate_and_score(ds, pipes)\n",
    "\n",
    "# df_predictions_multirc.to_csv(\"predicted_labels_all_models.csv\", index=False)\n",
    "print(df_predictions_multirc.head(), \"\\n\")\n",
    "\n",
    "for model_name, m in results.items():\n",
    "    print(f\"{model_name}:  acc={m['accuracy']:.4f}, f1_macro={m['f1_macro']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df42b0b-422f-48e8-bb8f-f8dbd4bd2101",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list(results.keys())\n",
    "accuracies = [results[m][\"accuracy\"] for m in models]\n",
    "f1s        = [results[m][\"f1_macro\"] for m in models]\n",
    "\n",
    "x = range(len(models))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure()\n",
    "plt.bar([xi - width/2 for xi in x], accuracies, width)\n",
    "plt.bar([xi + width/2 for xi in x], f1s, width)\n",
    "plt.xticks(x, models, rotation=30)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Accuracy and F1 (Macro) by Model\")\n",
    "plt.legend([\"Accuracy\", \"F1 (Macro)\"])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95623410-2284-4a3a-b1d7-61660369bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = sorted(df_predictions_multirc[\"true_label\"].unique())\n",
    "\n",
    "for name in models:\n",
    "    cm = confusion_matrix(\n",
    "        df_predictions_multirc[\"true_label\"],\n",
    "        df_predictions_multirc[f\"{name}_pred\"],\n",
    "        labels=labels\n",
    "    )\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, aspect='auto')\n",
    "    plt.xticks(range(len(labels)), labels, rotation=45)\n",
    "    plt.yticks(range(len(labels)), labels)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(f\"Confusion Matrix: {name}\")\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335e30fe-b165-4ac9-85ac-c0c82365ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def generate_classification_reports(df, models):\n",
    "    \n",
    "    reports = {}\n",
    "\n",
    "    for model in models:\n",
    "        preds = df[f\"{model}_pred\"]\n",
    "        true_labels = df[\"true_label\"]\n",
    "\n",
    "        report = classification_report(true_labels, preds, output_dict=True)\n",
    "        reports[model] = report\n",
    "\n",
    "        print(f\"\\nClassification Report for {model}:\\n\")\n",
    "        print(classification_report(true_labels, preds))\n",
    "\n",
    "    return reports\n",
    "\n",
    "classification_reports = generate_classification_reports(df_predictions_multirc, models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd20587f-67ff-4273-9707-7c9fee2afcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique labels after binarization: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# HUMAN-SCORED METRICS\n",
    "\n",
    "# pip install openpyxl\n",
    "import re\n",
    "\n",
    "sheet_id   = \"1qZc2b8wWlIRhxDr6Z9r6tzOcLsBrLXmKBvj4zp7-qHU\"\n",
    "export_url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=xlsx\"\n",
    "\n",
    "all_sheets = pd.read_excel(export_url, sheet_name=None)\n",
    "\n",
    "filtered = {\n",
    "    name: df for name, df in all_sheets.items()\n",
    "    if not re.fullmatch(r\"\\d+\", name)\n",
    "}\n",
    "\n",
    "dfs = []\n",
    "for name, df in filtered.items():\n",
    "    d = df.copy()\n",
    "    d[\"sheet_name\"] = name\n",
    "    dfs.append(d)\n",
    "combined = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# rename and binarize\n",
    "combined = combined.rename(\n",
    "    columns={\"reference_answer\": \"reference\", \"chunk_text\": \"text\", \"score (1-4)\": \"labels\"}\n",
    ")\n",
    "combined[\"labels\"] = (combined[\"labels\"] >= 3).astype(int)\n",
    "\n",
    "sheet_human_scored = {\n",
    "    \"test\": combined.to_dict(orient=\"records\")\n",
    "}\n",
    "\n",
    "# check label space\n",
    "print(\"\\nUnique labels after binarization:\", combined[\"labels\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0e408d-b003-45c5-bbd2-9c57906716fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_scored_preds, sheet_metrics = evaluate_and_score(sheet_human_scored, pipes)\n",
    "\n",
    "print(human_scored_preds.head(), \"\\n\")\n",
    "\n",
    "for model_name, m in sheet_metrics.items():\n",
    "    print(f\"{model_name}:  acc={m['accuracy']:.4f}, f1_macro={m['f1_macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd1cec4-8fc9-47ff-9fff-5cf81eb867cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list(sheet_metrics.keys())\n",
    "accuracies = [sheet_metrics[m][\"accuracy\"] for m in models]\n",
    "f1s        = [sheet_metrics[m][\"f1_macro\"] for m in models]\n",
    "\n",
    "x = range(len(models))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure()\n",
    "plt.bar([xi - width/2 for xi in x], accuracies, width)\n",
    "plt.bar([xi + width/2 for xi in x], f1s, width)\n",
    "plt.xticks(x, models, rotation=30)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Accuracy and F1 (Macro) by Model\")\n",
    "plt.legend([\"Accuracy\", \"F1 (Macro)\"])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401e7728-8db5-4ccc-881b-7371ee49c242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = sorted(human_scored_preds[\"true_label\"].unique())\n",
    "\n",
    "for name in models:\n",
    "    cm = confusion_matrix(\n",
    "        df_predictions_multirc[\"true_label\"],\n",
    "        df_predictions_multirc[f\"{name}_pred\"],\n",
    "        labels=labels\n",
    "    )\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, aspect='auto')\n",
    "    plt.xticks(range(len(labels)), labels, rotation=45)\n",
    "    plt.yticks(range(len(labels)), labels)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(f\"Confusion Matrix: {name}\")\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a251ff-d0e6-426f-900f-8c124db5021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_reports_humanscored = generate_classification_reports(human_scored_preds, models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metrics_env]",
   "language": "python",
   "name": "conda-env-metrics_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
