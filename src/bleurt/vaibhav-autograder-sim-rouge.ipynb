{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226d32d2-c8d6-43b0-b569-1cd85d5bfb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "from nltk import ngrams as nltk_ngrams\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix, classification_report\n",
    "\n",
    "import evaluate\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6e0754-29bb-45a9-865b-905fd1d8d52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_metric = evaluate.load('precision')\n",
    "recall_metric = evaluate.load('recall')\n",
    "\n",
    "rouge_model = evaluate.load('rouge')\n",
    "# bleu_model = evaluate.load(\"bleu\")\n",
    "# bleurt_model = evaluate.load(\"bleurt\", module_type=\"metric\")\n",
    "\n",
    "spacy_model = spacy.load('en_core_web_sm')\n",
    "similarity_model = SentenceTransformer('stsb-roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b9013-85e1-4335-b83a-12b2ce1d5825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pegasus_tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-large')\n",
    "# pegasus_model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-large')\n",
    "\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "def generate_summary(tokenizer, model, text):\n",
    "\n",
    "    text = f\"Summarize: {text}\"\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\", truncation=True)\n",
    "    \n",
    "    summary_ids = model.generate(inputs, max_length=150, num_beams=4, length_penalty=2.0, early_stopping=True, temperature=0.1, repetition_penalty=2.0)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b269d-7313-4a9f-badd-9efaaeb6c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/active-projects/textbook-question-generation/data/aqag-chatgpt-vicuna.csv')\n",
    "# df.rename({'correct_answer_vicuna': 'gold_answer'}, axis=1, inplace=True)\n",
    "df.rename({'correct_answer': 'gold_answer'}, axis=1, inplace=True)\n",
    "df.rename({'correct_answer_vicuna': 'correct_answer'}, axis=1, inplace=True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0588c523-931d-4716-87af-979d69c91af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape, df.dropna(subset=['clean_text', 'question', 'correct_answer', 'incorrect_answer', 'gold_answer']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf5c883-3caa-4aa9-a311-7f070b179383",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['clean_text', 'question', 'correct_answer', 'incorrect_answer', 'gold_answer'], inplace=True)\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b845ea36-2187-44fc-9f78-f94011781315",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_correct_df = df[['clean_text', 'question', 'gold_answer', 'correct_answer']]\n",
    "temp_correct_df.rename({'correct_answer': 'student_response'}, axis=1, inplace=True)\n",
    "temp_correct_df['true_label'] = 1\n",
    "# temp_correct_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d3a941-6899-497e-9252-a4b504f0b799",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_incorrect_df = df[['clean_text', 'question', 'gold_answer', 'incorrect_answer']]\n",
    "temp_incorrect_df.rename({'incorrect_answer': 'student_response'}, axis=1, inplace=True)\n",
    "temp_incorrect_df['true_label'] = 0\n",
    "# temp_incorrect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378d7efc-159d-47fc-9ea9-df8e3a918703",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape, temp_correct_df.shape, temp_incorrect_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a992a4f-570e-47a1-8e6a-efe90354d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([temp_correct_df, temp_incorrect_df]).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56245cd-d5ab-4664-a132-4e76fee77f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gold_answer'] = df['gold_answer'].apply(lambda x: generate_summary(t5_tokenizer, t5_model, x))\n",
    "df['student_response'] = df['student_response'].apply(lambda x: generate_summary(t5_tokenizer, t5_model, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096c0d35-f737-4fee-95dd-743bae8c48cf",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3465726-3645-44ef-8721-c7aa6cd6c9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing steps for similarity computation:\n",
    "# 1. lower case\n",
    "# 2. remove non-alphanumeric characters except those bringing in context - (['@', '#', '$', '%', '*', '<', '>', '.', ','])\n",
    "# 3. remove stopwords\n",
    "# 4. lemmatize --- experiment\n",
    "\n",
    "def func_preprocessing(text:str, lemmatize:bool=False):\n",
    "\n",
    "    return_list = list()\n",
    "    spacy_document = spacy_model(text.lower().strip())\n",
    "    for token in [token for token in spacy_document]:\n",
    "        if (token.text.isalnum() or any(i in token.text and token.text.count(i) == 1 for i in ['@', '#', '$', '%', '<', '>', '.', ',', '+', '-', '*'])) and (not token.is_stop):\n",
    "            if lemmatize:\n",
    "                return_list.append(token.lemma_)\n",
    "            else:\n",
    "                return_list.append(token.text)\n",
    "    \n",
    "    return ' '.join(return_list)\n",
    "\n",
    "df['processed_gold_answer'] = df['gold_answer'].apply(lambda x: func_preprocessing(x))\n",
    "df['processed_student_response'] = df['student_response'].apply(lambda x: func_preprocessing(x))\n",
    "\n",
    "df['processed_gold_answer_lemmatized'] = df['gold_answer'].apply(lambda x: func_preprocessing(x, lemmatize=True))\n",
    "df['processed_student_response_lemmatized'] = df['student_response'].apply(lambda x: func_preprocessing(x, lemmatize=True))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9c9906-ef91-4983-9d01-0d8b6955fbf1",
   "metadata": {},
   "source": [
    "#### Computing Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0ff157-7714-441a-a040-979048ff71a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing similarity between correct and incorect answer\n",
    "def compute_similarity(list_answers: list):\n",
    "    # calculating embeddings for the list -> [correct answer, incorrect answer]\n",
    "    embeddings = similarity_model.encode(list_answers, batch_size=16)\n",
    "    # returning similarity\n",
    "    return util.pytorch_cos_sim(embeddings[0], embeddings[1])[0].item()\n",
    "\n",
    "df['processed_similarity'] = df.apply(lambda x: compute_similarity([x['processed_gold_answer'], x['processed_student_response']]), axis=1)\n",
    "df['processed_lemmatized_similarity'] = df.apply(lambda x: compute_similarity([x['processed_gold_answer_lemmatized'], x['processed_student_response_lemmatized']]), axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443e8bc5-62c8-4192-ad6f-1ffe2228fc5e",
   "metadata": {},
   "source": [
    "#### Computing ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdc3e66-8d8b-464f-b445-b910fbabbbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing rouge_n. n is calculated as follows - \n",
    "# minimum of (9 OR 'half of number of tokens in correct answer' OR 'half of number of tokens in incorrect answer') - including 9 because that is the highest that evaluate library can compute\n",
    "# maximum of (1 OR the output of above) - to prevent n from being equal to 0.\n",
    "\n",
    "compute_rouge = lambda predictions, references, n: rouge_model.compute(predictions=[predictions], references=[references], rouge_types=[f'rouge{n}'])\n",
    "get_n = lambda t1, t2: max(1, min(9, int(len(t1.split()) / 2), int(len(t2.split()) / 2)))\n",
    "\n",
    "df['processed_rouge'] = df.apply(lambda x: list(compute_rouge(x['processed_student_response'], x['processed_gold_answer'], get_n(x['processed_student_response'], x['processed_gold_answer'])).values())[0], axis=1)\n",
    "df['processed_lemmatized_rouge'] = df.apply(lambda x: list(compute_rouge(x['processed_student_response_lemmatized'], x['processed_gold_answer_lemmatized'], get_n(x['processed_student_response_lemmatized'], x['processed_gold_answer_lemmatized'])).values())[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfc76dc-13d6-4693-9067-627ee914aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74616e2b-e063-41f1-abea-6c808c75c8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('~/active-projects/textbook-question-generation/data/aqag-chatgpt-vicuna-with-rouge-and-sim-vga.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753f1511-7c86-4114-b914-40d9b05fa35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_rouge'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d0d144-b475-45b0-9a87-a215b645b530",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_lemmatized_rouge'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0500e4f6-7c16-4969-897f-a261dba6f8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_similarity'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34e2f10-6ece-414f-99fc-e052551a0354",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_lemmatized_similarity'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5ed356-a6bc-49d8-8927-8617f4d88b83",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1339924-70f0-4592-89c0-6adb3ba0a3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring method -\n",
    "# 1. if similarity > 0.90 and rouge > 0.90 -> mark as correct\n",
    "# 2. if similarity > 0.95 and rouge > 0.85 -> mark as correct\n",
    "# 3. if similarity > 0.85 and rouge > 0.95 -> mark as correct\n",
    "# 4. else incorrect\n",
    "# return 1 for correct and 0 for incorrect\n",
    "\n",
    "def score(similarity_score: float, rouge_score: float):\n",
    "    if (similarity_score >= 0.90 and rouge_score >= 0.90) or (similarity_score >= 0.95 and rouge_score >= 0.85) or (similarity_score >= 0.85 and rouge_score >= 0.95):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "df['processed_score'] = df.apply(lambda x: score(x['processed_similarity'], x['processed_rouge']), axis=1)\n",
    "df['processed_lemmatized_score'] = df.apply(lambda x: score(x['processed_lemmatized_similarity'], x['processed_lemmatized_rouge']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5120b7-5227-46e7-a290-af66ce046f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(df['true_label'], df['processed_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244c21a-800a-434a-8c28-a7e7d20bca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(df['true_label'], df['processed_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c0b3a0-5fd7-4ccd-86e2-5e3d8316b9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen_kappa_score(df['true_label'], df['processed_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b563387a-c42d-4ba0-bb3d-7c6e9024a1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(df['true_label'], df['processed_lemmatized_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3281a39e-c006-43c4-be8f-87b73066ae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(df['true_label'], df['processed_lemmatized_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5765dd8-5ffb-4315-94b0-bded2f9a06eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen_kappa_score(df['true_label'], df['processed_lemmatized_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32287b04-cc2c-4678-a8f9-d18590038264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # scoring method -\n",
    "# # 1. if similarity > 0.90 and rouge > 0.90 -> mark as correct\n",
    "# # 2. if similarity > 0.95 and rouge > 0.85 -> mark as correct\n",
    "# # 3. if similarity > 0.85 and rouge > 0.95 -> mark as correct\n",
    "# # 4. else incorrect\n",
    "# # return 1 for correct and 0 for incorrect\n",
    "\n",
    "# def score(similarity_score: float, rouge_score: float):\n",
    "    \n",
    "#     for threshold in [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]:\n",
    "#     if (similarity_score >= 0.90 and rouge_score >= 0.90) or (similarity_score >= 0.95 and rouge_score >= 0.85) or (similarity_score >= 0.85 and rouge_score >= 0.95):\n",
    "#         return 1\n",
    "#     return 0\n",
    "\n",
    "# df['processed_score'] = df.apply(lambda x: score(x['processed_similarity'], x['processed_rouge']), axis=1)\n",
    "# df['processed_lemmatized_score'] = df.apply(lambda x: score(x['processed_lemmatized_similarity'], x['processed_lemmatized_rouge']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e455becd-0676-435f-b840-d54c9c54821f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_vaibhav_autograder]",
   "language": "python",
   "name": "conda-env-env_vaibhav_autograder-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
