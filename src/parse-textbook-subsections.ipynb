{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7def25-6115-4659-8ec7-c7c987790cae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "\n",
    "import difflib\n",
    "differ = difflib.HtmlDiff(wrapcolumn=100)\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from github_slugger import GithubSlugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c77f273-8d48-49c7-bd50-ea365be592bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONTENTS = Path('/home/jovyan/active-projects/macro-economics-textbook/contents/')\n",
    "# Get github slugger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12a1bdf-2e90-4996-8cb5-e537ccd1373b",
   "metadata": {},
   "source": [
    "## Get Markdown and Infer Metadata\n",
    "\n",
    "We infer metadata from the directory structure.  \n",
    "`PosixPath('../contents/module-3/index.mdx')` Each module has a root-level index page  \n",
    "`PosixPath('../contents/module-3/chapter-9/index.mdx')`  Each chapter has a root-level index page  \n",
    "`PosixPath('../contents/module-3/chapter-9/section-1/index.mdx')`  Each section has a content page, also called index.mdx  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3000b745-b0c2-4846-9515-a2424bf8b17d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mdx_sections = []\n",
    "\n",
    "verbose = False\n",
    "for path in sorted(CONTENTS.glob('**/index.mdx')):\n",
    "    rel_path = path.relative_to('/home/jovyan/active-projects/macro-economics-textbook/contents/')\n",
    "    parents = reversed(rel_path.parents[:-1]) # get all parent directories before '../contents'. We omit ([:-1]) the top-level relative directory '.'\n",
    "    module = next(parents).name.split('-')[1] # all .mdx files belong to a module\n",
    "    chapter = next(parents, Path('-0')).name.split('-')[1] # if the iterator is exhausted, this will make the chapter number 0\n",
    "    section = next(parents, Path('-0')).name.split('-')[1] # if the iterator is exhausted, this will make the section number 0\n",
    "    if verbose:\n",
    "        print(f'{path.as_posix():<60}Module {module}, Chapter {chapter}, Section {section}')\n",
    "    mdx_sections.append({\n",
    "        'module': module,\n",
    "        'chapter': chapter,\n",
    "        'section': section,\n",
    "        'path': path,\n",
    "    })\n",
    "\n",
    "mdx_sections[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e03676-ac81-41b4-beac-94be41e3d433",
   "metadata": {},
   "source": [
    "## Parse with regex\n",
    "\n",
    "We want to capture all content that is between two top-level headings.\n",
    "\n",
    "We also want to exclude certain subsections that have little or no text content.\n",
    "\n",
    "I hate regex so much. Let's break down the pattern below:  \n",
    "1. `(?:^#{1,2} )` A non-capturing group that looks for a line that starts with 1 or 2 '#', and ensures that the next character is a space ' '\n",
    "2. `(.*?)$` A capturing group that includes all characters until the end of the line. This is our subsection heading.\n",
    "3. `\\s*` matches on any any whitespace (optionally)\n",
    "4. `(.*?)` A capturing group that includes all characters. This is the subsection text.\n",
    "5. `(?=\\s*^#|\\Z)` A negative lookahead that tells us when to stop capturing subsection text. It will stop when it finds another subsection heading or the end of the document. It will include all the whitespace preceding one of these terminating elements, preventing that from being included in the subsection text capture group.\n",
    "\n",
    "`re.DOTALL` allows the '.' character to match on newlines.  \n",
    "`re.MULTILINE` makes the '^' and '$' anchors match on the beginning/end of lines instead of the beginning/end of the document. We use '\\Z' to match the end of the document in multiline mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b95068-3d68-4ded-91c0-5c20514e8c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'(?:^#{1,2} )(.*?)$\\s*(.*?)(?=\\s*^#|\\Z)', re.DOTALL | re.MULTILINE)\n",
    "\n",
    "# all these are lowercased because capitalization is inconsistent across MDX files\n",
    "subsections_to_skip = [\n",
    "    'learn with videos',\n",
    "    'please write your summary below',\n",
    "    'please your write summary below', # ... a perfect example of why this approach is doomed. More than 10 sections have this typo.\n",
    "    'bring it home',\n",
    "    'clear it up',\n",
    "    'work it out',\n",
    "]\n",
    "\n",
    "subsections = []\n",
    "\n",
    "verbose = True\n",
    "for section in mdx_sections:\n",
    "    text = section['path'].read_text()\n",
    "    matches = pattern.findall(text)\n",
    "    for i, match in enumerate(matches):\n",
    "        subsection_title = match[0]\n",
    "        subsection_text = match[1]\n",
    "        if subsection_title.lower().strip() in subsections_to_skip: # lowercase() and strip() because capitalization and spacing are inconsistent\n",
    "            if verbose and len(subsection_text) > 10: # if verbose, print the longer sections that we will be EXcluding\n",
    "                print('-'*80)\n",
    "                print(subsection_title, '\\n', subsection_text)\n",
    "                print('_'*80)                \n",
    "            continue\n",
    "        elif verbose and len(subsection_text) < 100: # if verbose, print the shorter sections that we will still be INcluding\n",
    "            print('-'*80)\n",
    "            print(subsection_title, '\\n', subsection_text)\n",
    "            print('_'*80)\n",
    "        else:\n",
    "            subsection_dict = {\n",
    "                **section, # add in section-level metadata\n",
    "                'subsection': i,\n",
    "                'heading': subsection_title,\n",
    "                'raw_text': subsection_text,\n",
    "            }\n",
    "            subsection_dict.pop('path')\n",
    "            subsections.append(subsection_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5931734-1393-4c2c-a8fa-2f57145c9062",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(subsections)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f263dab-ffbb-41f7-8f05-4aaec699c0db",
   "metadata": {},
   "source": [
    "## HTML Cleanup\n",
    "\n",
    "First step is to clean up the markdown files into something closer to standard markdown.\n",
    "\n",
    "RegEx is used to delete tables and their contents. Without the HTML tags, tables become a mangled list of strings.  \n",
    "We also use RegEx to remove the javascript import statements and convert links to standard markdown formatting.\n",
    "\n",
    "Next, we convert the markdown to HTML and use BeautifulSoup to extract the text.\n",
    "\n",
    "Debugging and testing is done with difflib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1891b63-2bb1-417e-ad05-bf68d155e1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(text, clean_text):\n",
    "    text_lines = [s.strip() for s in text.splitlines() if s.strip()] # delete empty lines\n",
    "    clean_text_lines = [s.strip() for s in clean_text.splitlines() if s.strip()] # delete empty lines\n",
    "    diff = differ.make_file(text_lines, clean_text_lines, fromdesc='Original', todesc='HTML Parsed', context=False, numlines=0)\n",
    "    display(HTML(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408ffeea-d1ab-42af-bea4-c17a7328b5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclamation_links_pattern = re.compile(r'!\\[') # Removes the exclamation point in links ![link_text](url) --> [link_text](url)\n",
    "html_table_pattern = re.compile(r'<Table.*?</Table>\\s*', re.DOTALL | re.IGNORECASE) # remove table HTML and its contents\n",
    "javascript_import_pattern = re.compile(r'^import.*?;', re.MULTILINE) # remove javascript imports\n",
    "\n",
    "def clean_md(text_md):\n",
    "    text_md = exclamation_links_pattern.sub(r'[', text_md)\n",
    "    text_md = html_table_pattern.sub('', text_md)\n",
    "    text_md = javascript_import_pattern.sub('', text_md)\n",
    "    return text_md.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd03e2e-c12d-4b95-befe-57a4ae589b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_raw_text(subsection_text_mdx):\n",
    "    subsection_text_md = clean_md(subsection_text_mdx)\n",
    "    subsection_text_html = markdown.markdown(subsection_text_md, extensions=['extra', 'tables'])\n",
    "    subsection_text_html = html_table_pattern.sub('', subsection_text_html) # this gets any markdown tables that have now been converted to HTML\n",
    "    return BeautifulSoup(subsection_text_html, features='html.parser').get_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c16e887-3947-4974-9e11-e809a935b0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in df.raw_text.sample(5):\n",
    "    clean_text = clean_raw_text(text)\n",
    "    diff(text, clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e0f05b-e868-407d-b4d8-96abe506a0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df.raw_text.apply(clean_raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d453ffe9-5ee5-4514-8e5f-cbaf9fe102e6",
   "metadata": {},
   "source": [
    "## Generate Slugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775fb7ad-c90e-4f0c-93d7-40fe87383fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_slug_github(group):\n",
    "    slugger = GithubSlugger()\n",
    "    group['slug'] = group['heading'].apply(slugger.slug)\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b552ef3f-d3a9-46b9-8367-252aa3edf183",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby(['module', 'chapter', 'section'], group_keys=False).apply(make_slug_github)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f37c105-850b-4d20-8181-bf17ca24d4f9",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7311576d-30b6-4ad3-ad05-0e8f435ee50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/subsections.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keyword-extraction]",
   "language": "python",
   "name": "conda-env-keyword-extraction-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
