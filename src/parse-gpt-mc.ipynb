{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "789b4b01-f705-4953-84ca-0c7eebb7f2fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this parses and stores the output as a dataframe. The correct prompt is:\n",
    "# Generate a multiple-choice question for the following passage. First, write 'Question:' and write the question. Then write the answer options. Then write 'Correct Answer:' and give the correct answer. Finally, write explanations for the correct answer and each of the distractors on a new line starting with 'Explanation for A'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d3113ff-d3b3-46bf-bf66-2ff204a9f109",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import openai\n",
    "import os\n",
    "from getpass import getpass\n",
    "import json\n",
    "\n",
    "openai_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ed3a77a-2ba7-4bdb-94a3-f0fe76bd3000",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA = '../data/'\n",
    "subsections = pd.read_csv(DATA + 'subsections.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c866c930-d574-440d-9002-69e282aa6dda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "model = OpenAI(temperature=0.9, openai_api_key = openai_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6274421a-7c80-44f6-a48a-6583ec6ceceb",
   "metadata": {},
   "source": [
    "## Short Answer Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff25005d-ed4a-4256-a8a7-f3021e90e081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The types of questions we are interested in\n",
    "kinds = ['recall', 'summarization', 'inference']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ca5b22f-732e-4eda-8728-bd179d9cc7f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a response schema to organize the output from the model\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"question\", description=\"a question based on the passage\"),\n",
    "    ResponseSchema(name=\"correct_answer\", description=\"a correct short answer to the question\"),\n",
    "    ResponseSchema(name=\"incorrect_answer\", description=\"an incorrect short answer to the question\")\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca310109-c337-40fb-b99d-b0280f79b711",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a prompt template, the input to feed to the model\n",
    "prompt = PromptTemplate(\n",
    "    template=\"generate a short answer {kind} question based on the passage. \\n{format_instructions}\\n{passage}\",\n",
    "    input_variables=[\"kind\", \"passage\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7014a9df-8ee3-495b-85e1-b56745a77760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This is a function which takes a passage and a desired type of question, as well as the chapter, section, and subsection numbers\n",
    "It returns a dataframe with one row\n",
    "'''\n",
    "\n",
    "def generate_sa_question(passage, kind, chapter=0, section=0, subsection=0, index = 0):\n",
    "    # Input to the model\n",
    "    _input = prompt.format_prompt(passage=passage, kind=kind)\n",
    "    output = model(_input.to_string())\n",
    "    \n",
    "    # Clean out the output string and convert it to a dataframe\n",
    "    output_string = output.replace('`','').replace('json','').strip()\n",
    "    output_dict = json.loads(output_string)\n",
    "    \n",
    "    # Add metadata to the dataframe\n",
    "    output_dict['clean_text'] = passage\n",
    "    output_dict['type'] = kind\n",
    "    output_dict['chapter'] = chapter\n",
    "    output_dict['section'] = section\n",
    "    output_dict['subsection'] = subsection\n",
    "    \n",
    "    return pd.DataFrame(output_dict, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b328977-6126-4c12-9465-883bc1d9a10c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "passage = subsections['clean_text'].iloc[0]\n",
    "kind = kinds[0]\n",
    "generate_sa_question(passage, kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d686ca6-33af-4e4d-884c-a6da8e7961d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This cell generates short answer questions and correct and incorrect answer for each row of the subsections dataframe\n",
    "'''\n",
    "\n",
    "df = pd.DataFrame({'clean_text':[], 'question':[], 'correct_answer':[], 'incorrect_answer':[], 'type':[], 'chapter':[], 'section':[], 'subsection':[]})\n",
    "# Count how many rows chatGPT failed at\n",
    "problem_rows = 0\n",
    "\n",
    "for row in subsections.iterrows():\n",
    "    print(row[0], end='\\r')\n",
    "    passage = row[1]['clean_text']\n",
    "    for kind in kinds:\n",
    "        try: \n",
    "            line = generate_sa_question(passage, kind, row[1]['chapter'], row[1]['section'], row[1]['subsection'])\n",
    "            df = pd.concat([df, line], axis=0)\n",
    "        except:\n",
    "            problem_rows += 1\n",
    "            continue\n",
    "\n",
    "print(f'Trouble parsing {problem_rows} subsections')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be071978-6339-4207-9a7b-2bc59ab0d54c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f26bc96-c3f2-4063-a73a-bb605d4c279f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.reset_index(drop=True).to_csv('../results/aqg.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785340eb-b374-4669-8562-30dca5b2bdf5",
   "metadata": {},
   "source": [
    "## Multiple Choice Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29d6328-8e92-4a11-bc9f-e2752969c14a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\r"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "I want you to generate a multiple choice question for a passage with one correct answer and explanations for all answers using this as an example:\n",
    "\n",
    "Question:\n",
    "\n",
    "A:\n",
    "B:\n",
    "C:\n",
    "D:\n",
    "\n",
    "Correct Answer:\n",
    "\n",
    "Explanation for A:\n",
    "Explanation for B:\n",
    "Explanation for C:\n",
    "Explanation for D:\n",
    "\n",
    "Here is the passage I want you to use to generate the question: {passage}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"passage\"],\n",
    "    template = template)\n",
    "\n",
    "questions_dict = {'id':[], 'chapter':[], 'section':[], 'subsection':[], 'item':[]}\n",
    "counter = 0\n",
    "for row in subsections.iterrows():\n",
    "    print(counter, end='\\r')\n",
    "    questions_dict['id'].append(row[0])\n",
    "    questions_dict['chapter'].append(row[1]['chapter'])\n",
    "    questions_dict['section'].append(row[1]['section'])\n",
    "    questions_dict['subsection'].append(row[1]['subsection'])\n",
    "    questions_dict['item'].append(llm(prompt.format(passage=row[1]['clean_text'])))\n",
    "    counter +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da128720-fad7-401e-af75-57286a82ae7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions_df = pd.DataFrame.from_dict(questions_dict)\n",
    "questions_df.to_csv('../data/gpt_questions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba12decb-98e2-4323-8136-2e7c1955f3ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_question(summary):\n",
    "    prompt = \"\"\"Generate a multiple-choice question, give the correct answer, and give explanations for the distractors for the following passage. Please use the following format:\n",
    "    Question: [Write the question here.] \n",
    "    A. [Write option A here]\n",
    "    B. [Write option B here]\n",
    "    C. [Write option C here]\n",
    "    D. [Write option D here]\n",
    "\n",
    "    Correct Answer: [Write the correct answer here] \n",
    "\n",
    "    Explanation for A: [Write the explanation for A here]\n",
    "    Explanation for B: [Write the explanation for B here]\n",
    "    Explanation for C: [Write the explanation for C here]\n",
    "    Explanation for D: [Write the explanation for D here]\n",
    "    \n",
    "\"\"\"\n",
    "    text_input = prompt + summary\n",
    "    completions = openai.Completion.create(engine=\"text-davinci-003\", prompt=text_input, max_tokens=2048, n=1,stop=None,temperature=0.7)\n",
    "    message = completions.choices[0].text\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e1ddbfa-3ad0-43f4-8a5e-d1618874392d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chatGPTparseMCQ(output):    \n",
    "    output_dict['question'].append(re.search('(Question:)[\\w\\d\\s]*/?', output).group(0)[10:])\n",
    "    output = re.sub('(Question:)[\\w\\d\\s]*/?', '', output).strip()\n",
    "\n",
    "    output_dict['answer_a'].append(re.search('A.+', output).group(0)[3:])\n",
    "    output = re.sub('A.+', '', output, count=1).strip()\n",
    "    output_dict['answer_b'].append(re.search('B.+', output).group(0)[3:])\n",
    "    output = re.sub('B.+', '', output, count=1).strip()\n",
    "    output_dict['answer_c'].append(re.search('C.+', output).group(0)[3:])\n",
    "    output = re.sub('C.+', '', output, count=1).strip()\n",
    "    output_dict['answer_d'].append(re.search('D.+', output).group(0)[3:])\n",
    "    output = re.sub('D.+', '', output, count=1).strip()\n",
    "\n",
    "    output_dict['correct_answer'].append(re.search('[Cc]orrect [Aa]nswer: .', output).group(0)[-1])\n",
    "    output = re.sub('[Cc]orrect [Aa]nswer: .+', '', output, count=1).strip()\n",
    "\n",
    "    a = re.search('Explanation for A.+', output)\n",
    "    if a == None:\n",
    "        output_dict['explanation_a'].append('')\n",
    "    else:\n",
    "        output_dict['explanation_a'].append(a.group(0)[18:])\n",
    "\n",
    "    b = re.search('Explanation for B.+', output)\n",
    "    if b == None:\n",
    "        output_dict['explanation_b'].append('')\n",
    "    else:\n",
    "        output_dict['explanation_b'].append(b.group(0)[18:])\n",
    "    \n",
    "    c = re.search('Explanation for C.+', output)\n",
    "    if c == None:\n",
    "        output_dict['explanation_c'].append('')\n",
    "    else:\n",
    "        output_dict['explanation_c'].append(c.group(0)[18:])\n",
    "        \n",
    "    d = re.search('Explanation for D.+', output)\n",
    "    if d == None:\n",
    "        output_dict['explanation_d'].append('')\n",
    "    else:\n",
    "        output_dict['explanation_d'].append(d.group(0)[18:])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fc6d91-b977-4cc4-b758-a7f38f7e68a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dict = {'chapter': [], 'section': [], 'subsection_number':[], 'subsection_slug': [], 'question': [], 'answer_a':[], 'answer_b':[], 'answer_c':[], 'answer_d':[], \n",
    "                'explanation_a':[], 'explanation_b':[], 'explanation_c':[], 'explanation_d':[], \n",
    "                'correct_answer':[]}\n",
    "\n",
    "for row in subsections.iterrows():\n",
    "    subsection = row[1]\n",
    "    output_dict['chapter'].append(subsection['chapter'])\n",
    "    output_dict['section'].append(subsection['section'])\n",
    "    output_dict['subsection_number'].append(subsection['subsection'])\n",
    "    output_dict['subsection_slug'].append(subsection['slug'])\n",
    "    output = generate_question(subsection['clean_text'])\n",
    "    print(output)\n",
    "    chatGPTparseMCQ(output)\n",
    "    \n",
    "pd.DataFrame.from_dict(output_dict).to_csv(DATA+'mcq_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada59ed5-70d0-4f63-978e-5e336b778f07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(output_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:WesleyEnv]",
   "language": "python",
   "name": "conda-env-WesleyEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
