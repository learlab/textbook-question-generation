{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cacf8f10-eaba-465b-b2d3-234a43572d87",
   "metadata": {},
   "source": [
    "# Generating text with LangChain and Huggingface\n",
    "\n",
    "We will start by setting up a standartd huggingface pipeline from our local Vicuna model. From there, it can be used as a normal Langchain LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07dede10-349b-4962-be63-4a1696ba2d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from transformers import pipeline, LlamaForCausalLM\n",
    "from accelerate import Accelerator\n",
    "import torch\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.base import Chain\n",
    "from langchain import PromptTemplate\n",
    "from langchain.output_parsers.regex_dict import RegexDictParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21396279-0b91-404d-91e0-ca5ebd03da94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be103b2768fc402396772f2df8d37ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_location = '/home/jovyan/project-archive/vicuna-7b'\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "        model_location,\n",
    "        load_in_8bit=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map={'': Accelerator().local_process_index},\n",
    "        max_length=4096\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f0fa730-85ef-401c-90dc-5635ae4f3ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(model=model,\n",
    "                tokenizer=model_location,\n",
    "                use_fast=False,\n",
    "                task='text-generation',\n",
    "                model_kwargs={'load_in_8bit': True},\n",
    "                max_length=2048,\n",
    "                temperature=0.9,\n",
    "                top_p=0.95,\n",
    "                repetition_penalty=1.1,\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2171019c-95cc-4d18-b824-ece5e41b685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b9d03a-5529-45d7-9d86-cc092bcadb7d",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b218723-9e4e-47d9-8b48-218dba33c7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/subsections.csv')\n",
    "subsections = df.clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541fc52e-6f59-4a6d-8f31-e39cdd3756ba",
   "metadata": {},
   "source": [
    "## Langchain\n",
    "\n",
    "The questions are looking fairly good. Now let's see if we can first extract the automatically generated questions reliably. Then, we will work on generating answers to those questions with the same model using LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004373d7-1ca0-4dd1-99f3-c919fcb2c3a8",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "368a486a-1283-459e-8b07-f7b96dcf82ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_template = (\n",
    "    #'The following is a passage from a macroeconomics textbook. \n",
    "    'Please provide an inference comprehension question about this passage to assess the learner\\'s understanding. '\n",
    "    'An inference comprehension question will ask the learner to make an educated guess or draw a conclusion based on the information presented in a passage or text. '\n",
    "    'The learner should be able to adequately answer the question in one or two sentences.\\n\\n'\n",
    "    '{source}\\n\\n'\n",
    "    'Inference Question:\\n\\n'\n",
    ")\n",
    "\n",
    "inference_prompt = PromptTemplate(\n",
    "    input_variables=['source'],\n",
    "    template=inference_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a8d0fc81-ebeb-4eb2-8477-c98ff987d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_template = (\n",
    "    #'The following is a passage from a macroeconomics textbook. \n",
    "    'Please provide a recall comprehension question about this passage to assess the learner\\'s understanding. '\n",
    "    'A recall comprehension question will ask the learner to remember specific details or information from the passage. '\n",
    "    'The learner should be able to adequately answer the question in one or two sentences.\\n\\n'\n",
    "    '{source}\\n\\n'\n",
    "    'Recall Question:\\n\\n'\n",
    ")\n",
    "\n",
    "recall_prompt = PromptTemplate(\n",
    "    input_variables=['source'],\n",
    "    template=recall_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5db4085-bb4e-429c-9c58-419ee8d83331",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_template = (\n",
    "    #'The following is a passage from a macroeconomics textbook. \n",
    "    'Please provide a summary comprehension question about this passage to assess the learner\\'s understanding. '\n",
    "    'A summary comprehension question will ask the learner to provide a brief overview of the main points or ideas in the passage. '\n",
    "    'The learner should be able to adequately answer the question in one or two sentences.\\n\\n'\n",
    "    '{source}\\n\\n'\n",
    "    'Summary Question:\\n\\n'\n",
    ")\n",
    "\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=['source'],\n",
    "    template=summary_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "42437c41-ea38-4080-a2f9-efd75c3aef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_chain = LLMChain(llm=llm, prompt=inference_prompt)\n",
    "recall_chain = LLMChain(llm=llm, prompt=recall_prompt)\n",
    "summary_chain = LLMChain(llm=llm, prompt=summary_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1c560908-fa4a-4e27-85c9-44efc8b542a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the aggregate supply (AS) curve?\\n\\nAnswer:\\n\\nThe aggregate supply (AS) curve is an upward sloping curve that shows the total quantity of output (real GDP) that firms will produce and sell at each price level.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_chain.run(subsections.sample().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "29e27fcc-7a57-4ce1-bccd-d0c0533b7c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_response_template = (\n",
    "    'The following is a passage from a macroeconomics textbook. Use the passage to generate a correct response to the question. '\n",
    "    'The response should fully and directly address the question with no conceptual or factual errors. '\n",
    "    'The response should be written in the voice of a student who has carefully read and understood the passage. '\n",
    "    'The response should be written in 1-2 complete sentences.\\n\\n'\n",
    "    'Passage:\\n{source}\\n\\n'\n",
    "    'Question:\\n{question}\\n\\n'\n",
    "    'Response:\\n'\n",
    ")\n",
    "\n",
    "correct_response_prompt = PromptTemplate(\n",
    "    input_variables=['source', 'question'],\n",
    "    template=correct_response_template,\n",
    ")\n",
    "\n",
    "correct_response_chain = LLMChain(llm=llm, prompt=correct_response_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "901b0fe6-5832-4fe2-bf8c-b7e0301ed53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_response_template = (\n",
    "    #'The following is a passage from a macroeconomics textbook. \n",
    "    'Use the passage to generate an incorrect response to the question. '\n",
    "    'The response will contain errors'#, or it may fail to directly address the question. '\n",
    "    'The response should be written in the voice of a student who has not fully understood the passage. It will be obviously incorrect. '\n",
    "    'The reponse should be written in 1 complete sentence.\\n\\n'\n",
    "    'Passage:\\n{source}\\n\\n'\n",
    "    'Question:\\n{question}\\n\\n'\n",
    "    'Response:\\n'\n",
    ")\n",
    "\n",
    "incorrect_response_prompt = PromptTemplate(\n",
    "    input_variables=['source', 'question'],\n",
    "    template=incorrect_response_template,\n",
    ")\n",
    "\n",
    "incorrect_response_chain = LLMChain(llm=llm, prompt=incorrect_response_prompt)\n",
    "\n",
    "# incorrect_response_template = (\n",
    "#     #'The following is a passage from a macroeconomics textbook. \n",
    "#     'Use the passage to generate an incorrect response to the question. '\n",
    "#     'The response will contain conceptual misunderstandings or factual errors'#, or it may fail to directly address the question. '\n",
    "#     'The response should be written in the voice of a student who has not fully understood the passage. It will be obviously incorrect. '\n",
    "#     'The reponse should be written in 1-2 complete sentences.\\n\\n'\n",
    "#     'Passage:\\n{source}\\n\\n'\n",
    "#     'Question:\\n{question}\\n\\n'\n",
    "#     'Response:\\n'\n",
    "# )\n",
    "\n",
    "# incorrect_response_prompt = PromptTemplate(\n",
    "#     input_variables=['source', 'question'],\n",
    "#     template=incorrect_response_template,\n",
    "# )\n",
    "\n",
    "# incorrect_response_chain = LLMChain(llm=llm, prompt=incorrect_response_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9601f136-bdd8-4a00-adf1-75bbe7df1aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = (\n",
    "    #'The following is a passage from a macroeconomics textbook. \n",
    "    'Use the passage to generate one correct and one incorrect response to the question. '\n",
    "    'The correct response will fully and directly address the question based on information from the passage. It will be free of conceptual and factual errors. '\n",
    "    # 'The incorrect response will contain conceptual misunderstandings or factual errors, or it will provide extraneous information that does not address the question. '\n",
    "    'The incorrect response will be obviously incorrect, as if written by a student who did not read the passage. '\n",
    "    'Both the correct and the incorrect responses should be written in 1-2 complete sentences.\\n\\n'\n",
    "    'Passage:\\n{source}\\n\\n'\n",
    "    'Question:\\n{question}\\n\\n'\n",
    "    'Correct Response:\\n'\n",
    ")\n",
    "\n",
    "response_prompt = PromptTemplate(\n",
    "    input_variables=['source', 'question'],\n",
    "    template=response_template,\n",
    ")\n",
    "\n",
    "response_chain = LLMChain(llm=llm, prompt=response_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2fe41bc6-fcc6-42c4-9b36-24bf6b19c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = subsections.sample().item()\n",
    "question = summary_chain.run(source)\n",
    "correct_response = correct_response_chain.run(source=source, question=question)\n",
    "incorrect_response = incorrect_response_chain.run(source=source, question=question)\n",
    "#response = response_chain.run(source=source, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ebae350d-bb36-4a07-994d-93b3c55eec9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unemployment is not distributed evenly across the U.S. population. Figure 7.3 shows unemployment rates broken down in various ways: by gender, age, and race/ethnicity.\n",
      "figure7.3a. Graph a shows the trends in unemployment rates by gender for the year 1972 to 2014. In 1972 the graph starts out at 6.6% for females. It jumps to 9.3% in 1975 for females, gradually goes back down until 2009, when it rises to 8.1%. It gradually lowers to 6.1% in 2014 for females. For males, it starts out at  around 5% in 1972, goes up and down periodically, and ends at 6.3% in 2014.  \n",
      "Figure 7.3 (a) Unemployment Rates by Gender (Source: www.bls.gov)\n",
      "--------------------------------------------------------------------------------\n",
      "Question: What are the trends in unemployment rates by gender from 1972 to 2014?\n",
      "--------------------------------------------------------------------------------\n",
      "Correct Response: The trends in unemployment rates by gender from 1972 to 2014 as shown in figure 7.3a are that for females, the rate starts at 6.6% in 1972 and gradually increases to 8.1% in 2009 before decreasing to 6.1% in 2014. For males, the rate starts at around 5% in 1972 and fluctuates over time before ending at 6.3% in 2014.\n",
      "--------------------------------------------------------------------------------\n",
      "Incorrect Response: Unemployment rates by gender have been steadily increasing for females since 1972, while fluctuating for males.\n"
     ]
    }
   ],
   "source": [
    "print(source)\n",
    "print('-'*80)\n",
    "print(f'Question: {question}')\n",
    "print('-'*80)\n",
    "print(f'Correct Response: {correct_response}')\n",
    "print('-'*80)\n",
    "print(f'Incorrect Response: {incorrect_response}')\n",
    "# print('-'*80)\n",
    "#print(f'Correct Response: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5348e3b1-ed1c-448b-927b-e045d4f6b61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensionQuestionChain(Chain):\n",
    "    summary_chain: LLMChain\n",
    "    recall_chain: LLMChain\n",
    "    inference_chain: LLMChain\n",
    "    correct_response_chain: LLMChain\n",
    "    incorrect_response_chain: LLMChain    \n",
    "\n",
    "    @property\n",
    "    def input_keys(self):\n",
    "        return ['source']\n",
    "\n",
    "    @property\n",
    "    def output_keys(self):\n",
    "        return [\n",
    "            'summary_question', 'summary_correct_response', 'summary_incorrect_response',\n",
    "            'recall_question', 'recall_correct_response', 'recall_incorrect_response',\n",
    "            'inference_question', 'inference_correct_response', 'inference_incorrect_response',\n",
    "        ]\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        summary_question = self.summary_chain.run(inputs)\n",
    "        recall_question = self.recall_chain.run(inputs)\n",
    "        inference_question = self.inference_chain.run(inputs)\n",
    "        ### I may need to parse the outputs of these chains...\n",
    "        return {\n",
    "            'summary_question': summary_question,\n",
    "            'summary_correct_response': self.correct_response_chain.run(source=source, question=summary_question),\n",
    "            'summary_incorrect_response': self.incorrect_response_chain.run(source=source, question=summary_question),\n",
    "            'recall_question': recall_question,\n",
    "            'recall_correct_response': self.correct_response_chain.run(source=source, question=recall_question),\n",
    "            'recall_incorrect_response': self.incorrect_response_chain.run(source=source, question=recall_question),\n",
    "            'inference_question': inference_question,\n",
    "            'inference_correct_response': self.correct_response_chain.run(source=source, question=inference_question),\n",
    "            'inference_incorrect_response': self.incorrect_response_chain.run(source=source, question=inference_question),\n",
    "        }\n",
    "\n",
    "comprehension_question_chain = ComprehensionQuestionChain(\n",
    "    summary_chain=summary_chain,\n",
    "    recall_chain=recall_chain,\n",
    "    inference_chain=inference_chain,\n",
    "    correct_response_chain=correct_response_chain,\n",
    "    incorrect_response_chain=incorrect_response_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b9832-b5dc-4133-92cb-e2f2b617bbbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e9f94a5e3f489eb85d4e7bc9a7ebba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/523 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/conda_envs/peft/lib/python3.10/site-packages/transformers/pipelines/base.py:1081: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Input length of input_ids is 2259, but `max_length` is set to 2048. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 2254, but `max_length` is set to 2048. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 2264, but `max_length` is set to 2048. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 2277, but `max_length` is set to 2048. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 2272, but `max_length` is set to 2048. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 2282, but `max_length` is set to 2048. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "def get_output(batch):\n",
    "    return comprehension_question_chain(batch['raw_text'])\n",
    "    \n",
    "\n",
    "ds = datasets.Dataset.from_pandas(df)\n",
    "ds1 = ds.map(get_output, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5bc860-1e68-43ee-9866-aae543fbe90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1.to_pandas().drop(columns = ['__index_level_0__', 'source']).to_csv('../results/vicuna_aqg.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8b567f-62de-45ba-a2f5-12cd5b75a7f8",
   "metadata": {},
   "source": [
    "### Output Parser\n",
    "\n",
    "Langchain reallys wants us to use a JSON or Pydantic parser. I highly doubt LLaMA-7B can reliably output structured responses. Let's try to build something with regex that fails gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1ce842da-4c2d-472a-9220-93f1a4617627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegexParser(RegexDictParser):\n",
    "    '''Overriding the parse method so that it does not escape regex patterns.\n",
    "    I need to match the first question at the beginning of the string with the regex '^' special character'''\n",
    "    def parse(self, text):\n",
    "        result = {}\n",
    "        for output_key, expected_format in self.output_key_to_format.items():\n",
    "            specific_regex = self.regex_pattern.format(expected_format)\n",
    "            matches = re.findall(specific_regex, text)\n",
    "            if not matches:\n",
    "                print(\n",
    "                    f\"No match found for output key: {output_key} with expected format ```{expected_format}``` on text ```{text.strip()}```\"\n",
    "                )\n",
    "                result[output_key] = '' # we can add in a retry function to try again if the model fails. for now, we will just return an empty string.\n",
    "            elif len(matches) > 1:\n",
    "                raise ValueError(\n",
    "                    f\"Multiple matches found for output key: {output_key} with expected format ```{expected_format}``` on text ```{text.strip()}```\"\n",
    "                )\n",
    "            elif (\n",
    "                self.no_update_value is not None and matches[0] == self.no_update_value\n",
    "            ):\n",
    "                continue\n",
    "            else:\n",
    "                result[output_key] = matches[0]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "33aead18-5689-4d00-a6f5-76e0b616d6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_key_to_format = {'Question 1': '^', # for the first question, we need to match the beginning of the string.\n",
    "                        'Question 2': 'Question 2:'}\n",
    "\n",
    "re_parser = RegexParser(\n",
    "    regex_pattern=r'{}\\s*(.*?)(?=\\n|$)', # searches for the key, a colon, any whitespace, and then matches on all the characters that follow until a linebreak or the end of string.\n",
    "    output_key_to_format=output_key_to_format,\n",
    "    no_update_value='N/A'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6a044ca1-774b-4e74-9d4b-c4babca2c19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed Output: {'Question 1': 'What does the author mean by \"demand\" in the context of economics?', 'Question 2': 'According to the passage, what are the two key components that determine the shape of a demand curve?'}\n",
      "Parsed Output: {'Question 1': 'Why do economists consider the ability to pay when measuring demand?', 'Question 2': 'How does the law of demand relate to the price and quantity demanded of a good or service?'}\n",
      "Parsed Output: {'Question 1': 'What is the main argument presented in this passage?', 'Question 2': 'How might high-income countries influence low-income countries to adopt stronger environmental standards without resorting to protectionism?'}\n",
      "Parsed Output: {'Question 1': 'What is the main idea of the passage?', 'Question 2': 'Why should sunk costs not affect the current decision according to the budget constraint framework?'}\n",
      "Parsed Output: {'Question 1': 'What is the difference between the aggregate supply and aggregate demand model and the microeconomic analysis of demand and supply in particular markets?', 'Question 2': 'Why does confusion sometimes arise between the two models?'}\n",
      "Parsed Output: {'Question 1': 'What are some of the reasons why studying economics is important?', 'Question 2': 'How does having a basic understanding of economics make someone a well-rounded thinker?'}\n",
      "Parsed Output: {'Question 1': 'What is the current average percentage of GDP spent by the federal government on research and development?', 'Question 2': 'According to the passage, what are some ways that fiscal policy can encourage R&D?'}\n",
      "Parsed Output: {'Question 1': 'What is the relationship between banks and the creation of money?', 'Question 2': 'How does the money multiplier formula relate to the creation of money by banks?'}\n",
      "Parsed Output: {'Question 1': 'What are some reasons why people can lose their jobs?', 'Question 2': 'How might new technology affect the number of workers needed in an industry?'}\n",
      "Parsed Output: {'Question 1': 'What is the main difference between nominal and real measurements?', 'Question 2': 'Why is it important to consider inflation when examining economic statistics?'}\n",
      "Parsed Output: {'Question 1': 'What are some examples of other supply shocks that could cause the aggregate supply curve to shift?', 'Question 2': 'How do these supply shocks affect the aggregate supply curve differently compared to changes in technology or changes in preferences?'}\n",
      "Parsed Output: {'Question 1': 'What happens to the supply curve when the cost of production goes up?', 'Question 2': 'How does the firm determine the price it wishes to charge?'}\n",
      "Parsed Output: {'Question 1': 'What is the main purpose of the demand and supply model?', 'Question 2': 'How can the demand and supply model explain changes in prices, wages, and rates of return?'}\n",
      "Parsed Output: {'Question 1': 'What are some examples of goods and services included in GDP?', 'Question 2': \"Why is measuring GDP important for evaluating the size of a nation's economy?\"}\n",
      "Parsed Output: {'Question 1': 'What factors contribute to the higher unemployment rates among younger workers?', 'Question 2': 'Why are the unemployment rates for black workers typically higher than those for white workers?'}\n"
     ]
    }
   ],
   "source": [
    "for sample in subsections.sample(15):\n",
    "    output = chain.run(sample)\n",
    "    try:\n",
    "        questions = re_parser.parse(output)\n",
    "        print('Parsed Output:', questions)\n",
    "    except ValueError as e:\n",
    "        print('Failed Parse:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26105d66-eb0a-4eef-b1fe-101c63d6ac27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:peft]",
   "language": "python",
   "name": "conda-env-peft-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
