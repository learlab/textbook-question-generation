{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cacf8f10-eaba-465b-b2d3-234a43572d87",
   "metadata": {},
   "source": [
    "# Generating text with LangChain and Huggingface\n",
    "\n",
    "We will start by setting up a standartd huggingface pipeline from our local Vicuna model. From there, it can be used as a normal Langchain LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07dede10-349b-4962-be63-4a1696ba2d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from transformers import pipeline, LlamaForCausalLM\n",
    "from accelerate import Accelerator\n",
    "import torch\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain\n",
    "from langchain import PromptTemplate\n",
    "from langchain.output_parsers.regex_dict import RegexDictParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21396279-0b91-404d-91e0-ca5ebd03da94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/jovyan/conda_envs/peft/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/jovyan/conda_envs/peft/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/jovyan/conda_envs/peft/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/conda_envs/peft/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/jovyan/conda_envs/peft/lib/libcudart.so.11.0'), PosixPath('/home/jovyan/conda_envs/peft/lib/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8583f9dd2b184c58aec29df32aac97cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_location = '/home/jovyan/project-archive/vicuna-7b'\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "        model_location,\n",
    "        load_in_8bit=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map={'': Accelerator().local_process_index}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f0fa730-85ef-401c-90dc-5635ae4f3ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(model=model,\n",
    "                tokenizer=model_location,\n",
    "                use_fast=False,\n",
    "                task='text-generation',\n",
    "                model_kwargs={'load_in_8bit': True},\n",
    "                max_length=2048,\n",
    "                temperature=0.6,\n",
    "                top_p=0.95,\n",
    "                repetition_penalty=1.1\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2171019c-95cc-4d18-b824-ece5e41b685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b9d03a-5529-45d7-9d86-cc092bcadb7d",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b218723-9e4e-47d9-8b48-218dba33c7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/subsections.csv')\n",
    "subsections = df.clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541fc52e-6f59-4a6d-8f31-e39cdd3756ba",
   "metadata": {},
   "source": [
    "## Langchain\n",
    "\n",
    "The questions are looking fairly good. Now let's see if we can first extract the automatically generated questions reliably. Then, we will work on generating answers to those questions with the same model using LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004373d7-1ca0-4dd1-99f3-c919fcb2c3a8",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "368a486a-1283-459e-8b07-f7b96dcf82ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_template = (\n",
    "    'In the field of education, there are three main types of comprehension questions: summary questions, recall questions, and inference questions. '\n",
    "    'The following is a passage from a macroeconomics textbook. Please provide an inference comprehension question to assess the learner\\'s understanding. '\n",
    "    'An inference comprehension question will ask the learner to make an educated guess or draw a conclusion based on the information presented in a passage or text. '\n",
    "    'The learner should be able to adequately answer the question in one or two sentences.\\n\\n'\n",
    "    '{source}\\n\\n'\n",
    "    'Inference Question:\\n\\n'\n",
    ")\n",
    "\n",
    "inference_prompt = PromptTemplate(\n",
    "    input_variables=['source'],\n",
    "    template=inference_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8d0fc81-ebeb-4eb2-8477-c98ff987d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_template = (\n",
    "    'In the field of education, there are three main types of comprehension questions: summary questions, recall questions, and inference questions. '\n",
    "    'The following is a passage from a macroeconomics textbook. Please provide a summary comprehension question to assess the learner\\'s understanding. '\n",
    "    'A recall comprehension question will ask the learner to remember specific details or information from the passage. '\n",
    "    'The learner should be able to adequately answer the question in one or two sentences.\\n\\n'\n",
    "    '{source}\\n\\n'\n",
    "    'Recall Question:\\n\\n'\n",
    ")\n",
    "\n",
    "recall_prompt = PromptTemplate(\n",
    "    input_variables=['source'],\n",
    "    template=recall_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5db4085-bb4e-429c-9c58-419ee8d83331",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_template = (\n",
    "    'In the field of education, there are three main types of comprehension questions: summary questions, recall questions, and inference questions. '\n",
    "    'The following is a passage from a macroeconomics textbook. Please provide a summary comprehension question to assess the learner\\'s understanding. '\n",
    "    'A summary comprehension question will ask the learner to provide a brief overview of the main points or ideas in the passage. '\n",
    "    'The learner should be able to adequately answer the question in one or two sentences.\\n\\n'\n",
    "    '{source}\\n\\n'\n",
    "    'Summary Question:\\n\\n'\n",
    ")\n",
    "\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=['source'],\n",
    "    template=summary_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42437c41-ea38-4080-a2f9-efd75c3aef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_chain = LLMChain(llm=llm, prompt=inference_prompt)\n",
    "recall_chain = LLMChain(llm=llm, prompt=recall_prompt)\n",
    "summary_chain = LLMChain(llm=llm, prompt=summary_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c560908-fa4a-4e27-85c9-44efc8b542a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What is the difference between the median weekly earnings for a full-time worker over 25 with no higher than a bachelor's degree and a high school diploma in 2015 according to the Bureau of Labor Statistics (BLS)?\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_chain.run(subsections.sample().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5348e3b1-ed1c-448b-927b-e045d4f6b61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.base import Chain\n",
    "\n",
    "class QuestionGenerationChain(Chain):\n",
    "    summary_chain: LLMChain\n",
    "    recall_chain: LLMChain\n",
    "    inference_chain: LLMChain\n",
    "\n",
    "    @property\n",
    "    def input_keys(self):\n",
    "        return ['source']\n",
    "\n",
    "    @property\n",
    "    def output_keys(self):\n",
    "        return ['Summary Question', 'Recall Question', 'Inference Question']\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return {\n",
    "            'Summary Question': self.summary_chain.run(inputs),\n",
    "            'Recall Question': self.recall_chain.run(inputs),\n",
    "            'Inference Question': self.inference_chain.run(inputs),\n",
    "        }\n",
    "\n",
    "question_generator = QuestionGenerationChain(summary_chain=summary_chain, recall_chain=recall_chain, inference_chain=inference_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3fb5b9a2-0255-4afc-9122-4ae994f56f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'In Chicago, Illinois, the highest recorded temperature was 105° in July 1995, while the lowest recorded temperature was 27° below zero in January 1958. Understanding why these extreme weather patterns occurred is certainly interesting. However, if you wanted to understand the typical weather pattern in Chicago, instead of focusing on one- time extremes, you would need to look at the entire pattern of data over time. A similar lesson applies to the study of macroeconomics. It is interesting to study extreme situations, like the 1930s Great Depression or the 2008-2009 Great Recession. If you want to understand the whole picture, however, you need to look at the long term using the neoclassical perspective.\\nConsider the unemployment rate. The unemployment rate has fluctuated from as low as 3.5% in 1969 to as high as 9.7% in 1982 and 9.6% in 2009. Even as the U.S. unemployment rate rose during recessions and declined during expansions, it kept returning to the general neighborhood of 5.0–5.5%. When the nonpartisan Congressional Budget Office carried out its long-range economic forecasts in 2010, it assumed that from 2015 to 2020, after the recession has passed, the unemployment rate would be 5.0%. From a long-run perspective, the economy seems to keep adjusting back to this rate of unemployment.\\nAs the name “neoclassical” implies, this perspective of how the macroeconomy works is a “new” view of the “old” classical model of the economy. The classical view, the predominant economic philosophy until the Great Depression, was that short-term fluctuations in economic activity would adjust back to full employment rather quickly, with flexible prices. This view of the economy implied a vertical aggregate supply curve at full employment GDP, and prescribed a “hands off” policy approach.\\nFor example, if the economy were to slip into recession (a leftward shift of the aggregate demand curve), it would temporarily exhibit a surplus of goods. Falling prices would eliminate this surplus, and the economy would return to full employment level of GDP. No active fiscal or monetary policy was needed. In fact, the classical view was that expansionary fiscal or monetary policy would only cause inflation, rather than increase GDP. The deep and lasting impact of the Great Depression changed this thinking and Keynesian economics, which prescribed active fiscal policy to alleviate weak aggregate demand, became the more mainstream perspective.',\n",
       " 'Summary Question': 'What are some reasons for studying extreme weather patterns and the long-term trends in unemployment rates?',\n",
       " 'Recall Question': 'What was the highest recorded temperature in Chicago, Illinois?',\n",
       " 'Inference Question': 'What can we learn about the typical weather pattern in Chicago by looking at the entire pattern of data over time?'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_generator(subsections.sample().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8b567f-62de-45ba-a2f5-12cd5b75a7f8",
   "metadata": {},
   "source": [
    "### Output Parser\n",
    "\n",
    "Langchain reallys wants us to use a JSON or Pydantic parser. I highly doubt LLaMA-7B can reliably output structured responses. Let's try to build something with regex that fails gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1ce842da-4c2d-472a-9220-93f1a4617627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegexParser(RegexDictParser):\n",
    "    '''Overriding the parse method so that it does not escape regex patterns.\n",
    "    I need to match the first question at the beginning of the string with the regex '^' special character'''\n",
    "    def parse(self, text):\n",
    "        result = {}\n",
    "        for output_key, expected_format in self.output_key_to_format.items():\n",
    "            specific_regex = self.regex_pattern.format(expected_format)\n",
    "            matches = re.findall(specific_regex, text)\n",
    "            if not matches:\n",
    "                print(\n",
    "                    f\"No match found for output key: {output_key} with expected format ```{expected_format}``` on text ```{text.strip()}```\"\n",
    "                )\n",
    "                result[output_key] = '' # we can add in a retry function to try again if the model fails. for now, we will just return an empty string.\n",
    "            elif len(matches) > 1:\n",
    "                raise ValueError(\n",
    "                    f\"Multiple matches found for output key: {output_key} with expected format ```{expected_format}``` on text ```{text.strip()}```\"\n",
    "                )\n",
    "            elif (\n",
    "                self.no_update_value is not None and matches[0] == self.no_update_value\n",
    "            ):\n",
    "                continue\n",
    "            else:\n",
    "                result[output_key] = matches[0]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "33aead18-5689-4d00-a6f5-76e0b616d6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_key_to_format = {'Question 1': '^', # for the first question, we need to match the beginning of the string.\n",
    "                        'Question 2': 'Question 2:'}\n",
    "\n",
    "re_parser = RegexParser(\n",
    "    regex_pattern=r'{}\\s*(.*?)(?=\\n|$)', # searches for the key, a colon, any whitespace, and then matches on all the characters that follow until a linebreak or the end of string.\n",
    "    output_key_to_format=output_key_to_format,\n",
    "    no_update_value='N/A'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6a044ca1-774b-4e74-9d4b-c4babca2c19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed Output: {'Question 1': 'What does the author mean by \"demand\" in the context of economics?', 'Question 2': 'According to the passage, what are the two key components that determine the shape of a demand curve?'}\n",
      "Parsed Output: {'Question 1': 'Why do economists consider the ability to pay when measuring demand?', 'Question 2': 'How does the law of demand relate to the price and quantity demanded of a good or service?'}\n",
      "Parsed Output: {'Question 1': 'What is the main argument presented in this passage?', 'Question 2': 'How might high-income countries influence low-income countries to adopt stronger environmental standards without resorting to protectionism?'}\n",
      "Parsed Output: {'Question 1': 'What is the main idea of the passage?', 'Question 2': 'Why should sunk costs not affect the current decision according to the budget constraint framework?'}\n",
      "Parsed Output: {'Question 1': 'What is the difference between the aggregate supply and aggregate demand model and the microeconomic analysis of demand and supply in particular markets?', 'Question 2': 'Why does confusion sometimes arise between the two models?'}\n",
      "Parsed Output: {'Question 1': 'What are some of the reasons why studying economics is important?', 'Question 2': 'How does having a basic understanding of economics make someone a well-rounded thinker?'}\n",
      "Parsed Output: {'Question 1': 'What is the current average percentage of GDP spent by the federal government on research and development?', 'Question 2': 'According to the passage, what are some ways that fiscal policy can encourage R&D?'}\n",
      "Parsed Output: {'Question 1': 'What is the relationship between banks and the creation of money?', 'Question 2': 'How does the money multiplier formula relate to the creation of money by banks?'}\n",
      "Parsed Output: {'Question 1': 'What are some reasons why people can lose their jobs?', 'Question 2': 'How might new technology affect the number of workers needed in an industry?'}\n",
      "Parsed Output: {'Question 1': 'What is the main difference between nominal and real measurements?', 'Question 2': 'Why is it important to consider inflation when examining economic statistics?'}\n",
      "Parsed Output: {'Question 1': 'What are some examples of other supply shocks that could cause the aggregate supply curve to shift?', 'Question 2': 'How do these supply shocks affect the aggregate supply curve differently compared to changes in technology or changes in preferences?'}\n",
      "Parsed Output: {'Question 1': 'What happens to the supply curve when the cost of production goes up?', 'Question 2': 'How does the firm determine the price it wishes to charge?'}\n",
      "Parsed Output: {'Question 1': 'What is the main purpose of the demand and supply model?', 'Question 2': 'How can the demand and supply model explain changes in prices, wages, and rates of return?'}\n",
      "Parsed Output: {'Question 1': 'What are some examples of goods and services included in GDP?', 'Question 2': \"Why is measuring GDP important for evaluating the size of a nation's economy?\"}\n",
      "Parsed Output: {'Question 1': 'What factors contribute to the higher unemployment rates among younger workers?', 'Question 2': 'Why are the unemployment rates for black workers typically higher than those for white workers?'}\n"
     ]
    }
   ],
   "source": [
    "for sample in subsections.sample(15):\n",
    "    output = chain.run(sample)\n",
    "    try:\n",
    "        questions = re_parser.parse(output)\n",
    "        print('Parsed Output:', questions)\n",
    "    except ValueError as e:\n",
    "        print('Failed Parse:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26105d66-eb0a-4eef-b1fe-101c63d6ac27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:peft]",
   "language": "python",
   "name": "conda-env-peft-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
